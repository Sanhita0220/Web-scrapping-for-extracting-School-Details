{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56ab197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab165a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped page 9\n",
      "Scraped page 10\n",
      "Scraped page 11\n",
      "Scraped page 12\n",
      "Scraped page 13\n",
      "Scraped page 14\n",
      "Scraping completed. URLs saved to 'ts_urls.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the URLs\n",
    "readmore_urls = []\n",
    "\n",
    "# Iterate over each page\n",
    "for i in range(1, 15):  # Assuming there are 24 pages\n",
    "    # Construct the URL for the current page\n",
    "    url = 'https://www.cbseschool.org/location/telangana/page/{}/'.format(i)\n",
    "    \n",
    "    # Define User-Agent header\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    # Send a GET request to the URL with headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all containers with class 'catbox'\n",
    "        containers = soup.find_all('div', class_='catbox')\n",
    "        \n",
    "        # Extract information from each container\n",
    "        for container in containers:\n",
    "            try:\n",
    "                # Find the 'Read More' link\n",
    "                readmore_link = container.find('a', class_='link')['href']\n",
    "                \n",
    "                # Append the URL to the list\n",
    "                readmore_urls.append(readmore_link)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "    \n",
    "    # Print progress message\n",
    "    print(\"Scraped page\", i)\n",
    "\n",
    "# Create a DataFrame from the list of URLs\n",
    "df = pd.DataFrame({'Read More URLs': readmore_urls})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('ts_urls.xlsx', index=False)\n",
    "\n",
    "print(\"Scraping completed. URLs saved to 'ts_urls.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f58e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f36a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
